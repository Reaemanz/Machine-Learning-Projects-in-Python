{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18784d60d3b7bdc2cf24a296519e9a93cb0c61fb"
   },
   "source": [
    "## General information\n",
    "\n",
    "In this kernel I'll work with data from Quora Insincere Questions Classification Competition.\n",
    "\n",
    "This dataset is interesting for NLP researching. We will try to find insincere questions which aren't usefull or are even harmful. I'll do a simple EDA and try an LSTM-CNN model. \n",
    "\n",
    "![](https://pbs.twimg.com/profile_images/1013607595616038912/pRq_huGc_400x400.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import time\n",
    "pd.set_option('max_colwidth',400)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, Masking\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "ab0fa74be8858a2b3197cf2761c85cc6964f5600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove.840B.300d.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"../input/embeddings/glove.840B.300d/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2b77f6a1c831c98851143feb25c9903cb1154bf2"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "sub = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28c5b2320f5ef863df3d0b4e4d9175c59bd61ef0"
   },
   "source": [
    "## Data overview\n",
    "\n",
    "This is a kernel competition, where we can't use external data. As a result we can use only train and test datasets as well as embeddings which were provided by organizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "1558909b0a5c120c1d5ddc5be4f5a952fcb4971e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available embeddings: ['paragram_300_sl999', 'glove.840B.300d', 'wiki-news-300d-1M', 'GoogleNews-vectors-negative300']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('Available embeddings:', os.listdir(\"../input/embeddings/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "0c7299c8895405ef00049595ead1ef89649ba71b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1225312\n",
       "1      80810\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cdbe2595e31608b72cfbdc8d4bfc75840bfe3a0d"
   },
   "source": [
    "We have a serious disbalance - only ~6% of data are positive. No wonder the metric for the competition is f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "afaa845d44d72b9997ce037ab547ab4010701311"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg hemispheres?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain bike by just changing the tyres?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  ...   target\n",
       "0  00002165364db923c7e6  ...        0\n",
       "1  000032939017120e6e44  ...        0\n",
       "2  0000412ca6e4628ce2cf  ...        0\n",
       "3  000042bf85aa498cd78e  ...        0\n",
       "4  0000455dfa3e01eae3af  ...        0\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0817c86624cc43ea1df0eba310ba41f4f799f8da"
   },
   "source": [
    "In the dataset we have only texts of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "54a553b7e92a2a0a3d491ccf92b437011b813c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of questions in train is 13.\n",
      "Average word length of questions in test is 13.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of questions in train is {0:.0f}.'.format(np.mean(train['question_text'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of questions in test is {0:.0f}.'.format(np.mean(test['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "7861669f72f36145c25911926a51bc688f51d473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word length of questions in train is 134.\n",
      "Max word length of questions in test is 87.\n"
     ]
    }
   ],
   "source": [
    "print('Max word length of questions in train is {0:.0f}.'.format(np.max(train['question_text'].apply(lambda x: len(x.split())))))\n",
    "print('Max word length of questions in test is {0:.0f}.'.format(np.max(test['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "1b1303137eb44cc0de3329921751c48209037562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of questions in train is 71.\n",
      "Average character length of questions in test is 70.\n"
     ]
    }
   ],
   "source": [
    "print('Average character length of questions in train is {0:.0f}.'.format(np.mean(train['question_text'].apply(lambda x: len(x)))))\n",
    "print('Average character length of questions in test is {0:.0f}.'.format(np.mean(test['question_text'].apply(lambda x: len(x)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b82f366ef8b46b0115e9940c7966849023545733"
   },
   "source": [
    "As we can see on average questions in train and test datasets are similar, but there are quite long questions in train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "9d081b2f0d46faf01a943c309568c27f92462f94"
   },
   "outputs": [],
   "source": [
    "max_features = 90000\n",
    "tk = Tokenizer(lower = True, filters='', num_words=max_features)\n",
    "full_text = list(train['question_text'].values) + list(test['question_text'].values)\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "33929a60e1872b73e40424daa1178a3d8fbf8f5a"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['question_text'].fillna('missing'))\n",
    "test_tokenized = tk.texts_to_sequences(test['question_text'].fillna('missing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "4330f6c01064b5fda4ca9661dc4f1cefb439cf75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Distribution of question text length in characters')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['question_text'].apply(lambda x: len(x.split())).plot(kind='hist');\n",
    "plt.yscale('log');\n",
    "plt.title('Distribution of question text length in characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "05c7e2c63afb343b64835432721a42f81dd53626"
   },
   "source": [
    "We can see that most of the questions are 40 words long or shorter. Let's try having sequence length equal to 70 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "0a3f7fc48edb7d8d4aec66042dfb45e5af225c44"
   },
   "outputs": [],
   "source": [
    "max_len = 70\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "744ee4a1fbc66cb47aae6a18f829dea284b860c9"
   },
   "outputs": [],
   "source": [
    "embedding_path = \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "#embedding_path = \"../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "5cd86c2400db5ea1511b107250c8aa8c98ea909b"
   },
   "outputs": [],
   "source": [
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "2948d63cafb76b446c5e167a6dce7915ca43da6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_path, encoding='utf-8', errors='ignore'))\n",
    "all_embs = np.stack(embedding_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "9011f3f1e1affbe0795e2f7a98d4f0e98a8c4925"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(train['target'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58c29867982a695f802700c77ebb213a52e18659"
   },
   "source": [
    "For now I'll use an architecture from my previous [kernel](https://www.kaggle.com/artgor/movie-review-sentiment-analysis-eda-and-models) in another competition.\n",
    "\n",
    "The architecture in the following:\n",
    "- input with embedding;\n",
    "- then we have separate \"branches\" - GRU and LSTM;\n",
    "- each \"branch\" is processed by two Conv1D layers separately;\n",
    "- each Conv1D layer has average and max pooling layers;\n",
    "- all pooling layers are concatenated;\n",
    "- two dense layers in the end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "55bc04ce8d311a8176886a211217d4e4459d4700"
   },
   "outputs": [],
   "source": [
    "def build_model(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32, epochs=20):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(max_features + 1, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
    "    \n",
    "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
    "    \n",
    "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
    "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
    "    \n",
    "    \n",
    "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
    "    \n",
    "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
    "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(2, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    model.summary()\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 512, epochs = epochs, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "90ac92a570fe3c180252dfcf1f22c5f98a08e615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 70, 300)      27000300    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 70, 300)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 70, 128)      140544      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 70, 128)      187392      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 67, 16)       8208        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 68, 16)       6160        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 67, 16)       8208        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 68, 16)       6160        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 16)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 16)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 16)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 16)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 16)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 16)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 16)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 16)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128)          512         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           2064        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16)           64          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 8)            136         batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 8)            0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            18          dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 27,359,766\n",
      "Trainable params: 359,178\n",
      "Non-trainable params: 27,000,588\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/5\n",
      "1175509/1175509 [==============================] - 296s 252us/step - loss: 0.3297 - acc: 0.8718 - val_loss: 0.1438 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14381, saving model to best_model.hdf5\n",
      "Epoch 2/5\n",
      "1175509/1175509 [==============================] - 291s 248us/step - loss: 0.1828 - acc: 0.9381 - val_loss: 0.1371 - val_acc: 0.9480\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14381 to 0.13708, saving model to best_model.hdf5\n",
      "Epoch 3/5\n",
      " 682496/1175509 [================>.............] - ETA: 1:57 - loss: 0.1657 - acc: 0.9405"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = build_model(lr = 1e-4, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=4, kernel_size2=3, dense_units=16, dr=0.1, conv_size=16, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "8b206256803deeb5ba630578da1175b3cc01f555"
   },
   "outputs": [],
   "source": [
    "# pred = model.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "# predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
    "# sub['prediction'] = predictions\n",
    "# sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "754211d82f222f50cbb19712aca94984b2191588"
   },
   "outputs": [],
   "source": [
    "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32, epochs=20):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(max_features + 1, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    \n",
    "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
    "    \n",
    "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
    "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
    "\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(2, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    model.summary()\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 512, epochs = epochs, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "552f593475f579d8c2822ee6c2d6b52ed1abc3e5"
   },
   "outputs": [],
   "source": [
    "#model1 = build_model1(lr = 1e-4, lr_d = 1e-7, units = 128, spatial_dr = 0.3, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.3, conv_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "0055ab6717740b4b234ada9aacb0c78074e64b55"
   },
   "outputs": [],
   "source": [
    "#model1_1 = build_model1(lr = 1e-4, lr_d = 1e-7, units = 128, spatial_dr = 0.3, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "836e8eec34c09dcca660d35f39ddd5ec4801147a"
   },
   "outputs": [],
   "source": [
    "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32, epochs=20):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(max_features + 1, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units * 2, return_sequences = True))(x1)\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x_gru)\n",
    "    \n",
    "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
    "    \n",
    "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
    "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
    "  \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(2, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    model.summary()\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 512, epochs = epochs, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "0284b87509f5b94a6d03db579be74dad0bc8ff73"
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#model2 = build_model2(lr = 1e-4, lr_d = 1e-7, units = 256, spatial_dr = 0.3, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "ac612d038ef7cf161c4e7c678751d29e4e2c7a82"
   },
   "outputs": [],
   "source": [
    "#model3 = build_model2(lr = 1e-3, lr_d = 1e-7, units = 256, spatial_dr = 0.3, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=16, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "3f6b13c224c17bf70eac09f9bc3846d8332fa8d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 70, 300)      27000300    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 70, 300)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 70, 256)      330240      spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 70, 128)      123648      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 67, 8)        4104        bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 68, 8)        3080        bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 8)            0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 8)            0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 8)            0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 8)            0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32)           0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32)           128         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           1056        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32)           128         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           528         batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            34          dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 27,463,246\n",
      "Trainable params: 462,818\n",
      "Non-trainable params: 27,000,428\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/5\n",
      "1175509/1175509 [==============================] - 334s 284us/step - loss: 0.2000 - acc: 0.9309 - val_loss: 0.1326 - val_acc: 0.9495\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13262, saving model to best_model.hdf5\n",
      "Epoch 2/5\n",
      "1175509/1175509 [==============================] - 331s 282us/step - loss: 0.1417 - acc: 0.9474 - val_loss: 0.1265 - val_acc: 0.9509\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13262 to 0.12647, saving model to best_model.hdf5\n",
      "Epoch 3/5\n",
      "1175509/1175509 [==============================] - 331s 281us/step - loss: 0.1333 - acc: 0.9498 - val_loss: 0.1282 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.12647\n",
      "Epoch 4/5\n",
      "1175509/1175509 [==============================] - 331s 282us/step - loss: 0.1276 - acc: 0.9515 - val_loss: 0.1202 - val_acc: 0.9530\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12647 to 0.12022, saving model to best_model.hdf5\n",
      "Epoch 5/5\n",
      "1175509/1175509 [==============================] - 332s 283us/step - loss: 0.1238 - acc: 0.9526 - val_loss: 0.1170 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.12022 to 0.11700, saving model to best_model.hdf5\n",
      "CPU times: user 18min 13s, sys: 6min 22s, total: 24min 35s\n",
      "Wall time: 27min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model4 = build_model2(lr = 1e-4, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=8, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "582cadcbd9fdd70e4c6da7160d743057fd9a487e"
   },
   "outputs": [],
   "source": [
    "#model5 = build_model2(lr = 1e-4, lr_d = 1e-7, units = 256, spatial_dr = 0.1, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=16, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04d92de6fc799aca62d3b41c588973d2480b2672"
   },
   "source": [
    "### Model with attention\n",
    "\n",
    "https://github.com/Diyago/ML-DL-scripts/blob/9e161a96580efa9993805ca28f610df72fe36406/DEEP%20LEARNING/LSTM%20RNN/Sentiment%20analysis%20LSTM%20wth%20Bidirectional%20%20%2B%20Custom%20Attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "0571d9edafb014eb480950a8c67fb892a4780240"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "143768540a1c3ad0783e49362e9018b7d46a1f07"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "19030bd2b6277b9e5dbc6933ca0abcf68fc7b3f9"
   },
   "outputs": [],
   "source": [
    "def build_model3(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, dense_units=128, dr=0.1, use_attention=True):\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(max_features + 1, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units * 2, return_sequences = True))(x1)\n",
    "    if use_attention:\n",
    "        x_att = Attention(max_len)(x_gru)\n",
    "        x = Dropout(dr)(Dense(dense_units, activation='relu') (x_att))\n",
    "    else:\n",
    "        x_att = Flatten() (x_gru)\n",
    "        x = Dropout(dr)(Dense(dense_units, activation='relu') (x_att))\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(2, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    #model.summary()\n",
    "    #history = model.fit(X_train, y_ohe, batch_size = 512, epochs = epochs, validation_split=0.1, \n",
    "    #                    verbose = 1, callbacks = [check_point, early_stop])\n",
    "    #model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "1e7072209b383437e44560d062c1614418a1607a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/10\n",
      "1175509/1175509 [==============================] - 204s 174us/step - loss: 0.1703 - acc: 0.9376 - val_loss: 0.1184 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11836, saving model to best_model.hdf5\n",
      "Epoch 2/10\n",
      "1175509/1175509 [==============================] - 201s 171us/step - loss: 0.1212 - acc: 0.9531 - val_loss: 0.1125 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11836 to 0.11252, saving model to best_model.hdf5\n",
      "Epoch 3/10\n",
      "1175509/1175509 [==============================] - 200s 170us/step - loss: 0.1159 - acc: 0.9546 - val_loss: 0.1101 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.11252 to 0.11013, saving model to best_model.hdf5\n",
      "Epoch 4/10\n",
      "1175509/1175509 [==============================] - 200s 170us/step - loss: 0.1120 - acc: 0.9558 - val_loss: 0.1081 - val_acc: 0.9567\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.11013 to 0.10810, saving model to best_model.hdf5\n",
      "Epoch 5/10\n",
      "1175509/1175509 [==============================] - 200s 170us/step - loss: 0.1090 - acc: 0.9567 - val_loss: 0.1078 - val_acc: 0.9572\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10810 to 0.10779, saving model to best_model.hdf5\n",
      "Epoch 6/10\n",
      " 506368/1175509 [===========>..................] - ETA: 1:49 - loss: 0.1063 - acc: 0.9576"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_path = \"best_model.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                              save_best_only = True, mode = \"min\")\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "model6 = build_model3(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, dense_units=16, dr=0.1, use_attention=True)\n",
    "history = model6.fit(X_train, y_ohe, batch_size = 512, epochs = 10, validation_split=0.1, \n",
    "                    verbose = 1, callbacks = [check_point, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "4c0cab2368cf13a33e56a92eec20ac01254c3529"
   },
   "outputs": [],
   "source": [
    "# #%%time\n",
    "# file_path = \"best_model.hdf5\"\n",
    "# check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "#                               save_best_only = True, mode = \"min\")\n",
    "# early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "# model6_1 = build_model3(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, dense_units=16, dr=0.1, use_attention=False)\n",
    "# history = model6_1.fit(X_train, y_ohe, batch_size = 512, epochs = 5, validation_split=0.1, \n",
    "#                     verbose = 1, callbacks = [check_point, early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bbc608b1a70e5bc21df515136b6225da29e797a9"
   },
   "source": [
    "### One branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "fecf3405fccd307de507cea500df6b559e70cc95"
   },
   "outputs": [],
   "source": [
    "def build_model4(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32, epochs=20):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(max_features + 1, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
    "    \n",
    "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
    "       \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(2, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    model.summary()\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 512, epochs = epochs, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "29fcf5b25f20763deb75612eac29d58add377fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 70, 300)      27000300    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_4 (SpatialDro (None, 70, 300)      0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 70, 128)      140544      spatial_dropout1d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 68, 8)        3080        bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 8)            0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 8)            0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16)           0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16)           64          concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 32)           544         batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32)           0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32)           128         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 2)            66          batch_normalization_7[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 27,144,726\n",
      "Trainable params: 144,330\n",
      "Non-trainable params: 27,000,396\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/5\n",
      "1175509/1175509 [==============================] - 146s 125us/step - loss: 0.4302 - acc: 0.8300 - val_loss: 0.1878 - val_acc: 0.9490\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18780, saving model to best_model.hdf5\n",
      "Epoch 2/5\n",
      "1175509/1175509 [==============================] - 143s 122us/step - loss: 0.1657 - acc: 0.9448 - val_loss: 0.1320 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.18780 to 0.13201, saving model to best_model.hdf5\n",
      "Epoch 3/5\n",
      "1175509/1175509 [==============================] - 143s 121us/step - loss: 0.1398 - acc: 0.9480 - val_loss: 0.1256 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13201 to 0.12564, saving model to best_model.hdf5\n",
      "Epoch 4/5\n",
      "1175509/1175509 [==============================] - 143s 121us/step - loss: 0.1319 - acc: 0.9495 - val_loss: 0.1223 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12564 to 0.12232, saving model to best_model.hdf5\n",
      "Epoch 5/5\n",
      "1175509/1175509 [==============================] - 142s 121us/step - loss: 0.1284 - acc: 0.9506 - val_loss: 0.1207 - val_acc: 0.9528\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.12232 to 0.12074, saving model to best_model.hdf5\n",
      "CPU times: user 8min 59s, sys: 2min 33s, total: 11min 33s\n",
      "Wall time: 12min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model7 = build_model4(lr = 1e-4, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=3, dense_units=32, dr=0.1, conv_size=8, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "1b21c1b068660aaca49e9f1d1c8259bd5c97ad56"
   },
   "outputs": [],
   "source": [
    "#model8 = build_model4(lr = 1e-4, lr_d = 1e-7, units = 128, spatial_dr = 0.3, kernel_size1=4, dense_units=32, dr=0.1, conv_size=8, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f12bb4ca1ee7bca3caa41426590330365e057d34"
   },
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "dcae5f568eac0945c349eb2253df8010d03faa97"
   },
   "outputs": [],
   "source": [
    "def build_model5(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32, epochs=20):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=2, min_lr=0.001)\n",
    "\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(max_features + 1, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "    x_m = Masking()(x1)\n",
    "    x_gru = LSTM(units)(x_m)\n",
    "\n",
    "    x = BatchNormalization()(x_gru)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(2, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    model.summary()\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 512, epochs = epochs, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop, reduce_lr])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "3b98e6c57a2f16b29b8ae7265919eb309e36e770"
   },
   "outputs": [],
   "source": [
    "#model9 = build_model5(lr = 1e-4, lr_d = 1e-7, units = 128, spatial_dr = 0.3, kernel_size1=4, dense_units=32, dr=0.1, conv_size=8, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "771278144d9fe02f509ee4e08b7a97d3e69b8283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56370/56370 [==============================] - 5s 90us/step\n",
      "56370/56370 [==============================] - 6s 104us/step\n",
      "56370/56370 [==============================] - 3s 56us/step\n",
      "56370/56370 [==============================] - 4s 71us/step\n"
     ]
    }
   ],
   "source": [
    "pred1 = model.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred = pred1\n",
    "pred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred4\n",
    "pred2 = model7.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred2\n",
    "# pred3 = model9.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "# pred += pred3\n",
    "pred4 = model6.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred4\n",
    "# pred5 = model7.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "# pred += pred5\n",
    "pred = pred / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "2c04e298f5c96a9561450874a0238f408984c39e"
   },
   "outputs": [],
   "source": [
    "#pred = model9.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "\n",
    "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
    "sub['prediction'] = predictions\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
