{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Demo for survival analysis (regression) with Optuna.\n\nDemo for survival analysis (regression) using Accelerated Failure Time (AFT) model,\nusing Optuna to tune hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport optuna\nimport pandas as pd\nfrom sklearn.model_selection import ShuffleSplit\n\nimport xgboost as xgb\n\n# The Veterans' Administration Lung Cancer Trial\n# The Statistical Analysis of Failure Time Data by Kalbfleisch J. and Prentice R (1980)\ndf = pd.read_csv('../data/veterans_lung_cancer.csv')\nprint('Training data:')\nprint(df)\n\n# Split features and labels\ny_lower_bound = df['Survival_label_lower_bound']\ny_upper_bound = df['Survival_label_upper_bound']\nX = df.drop(['Survival_label_lower_bound', 'Survival_label_upper_bound'], axis=1)\n\n# Split data into training and validation sets\nrs = ShuffleSplit(n_splits=2, test_size=.7, random_state=0)\ntrain_index, valid_index = next(rs.split(X))\ndtrain = xgb.DMatrix(X.values[train_index, :])\ndtrain.set_float_info('label_lower_bound', y_lower_bound[train_index])\ndtrain.set_float_info('label_upper_bound', y_upper_bound[train_index])\ndvalid = xgb.DMatrix(X.values[valid_index, :])\ndvalid.set_float_info('label_lower_bound', y_lower_bound[valid_index])\ndvalid.set_float_info('label_upper_bound', y_upper_bound[valid_index])\n\n# Define hyperparameter search space\nbase_params = {'verbosity': 0,\n              'objective': 'survival:aft',\n              'eval_metric': 'aft-nloglik',\n              'tree_method': 'hist'}  # Hyperparameters common to all trials\ndef objective(trial):\n    params = {'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n              'aft_loss_distribution': trial.suggest_categorical('aft_loss_distribution',\n                                                                  ['normal', 'logistic', 'extreme']),\n              'aft_loss_distribution_scale': trial.suggest_loguniform('aft_loss_distribution_scale', 0.1, 10.0),\n              'max_depth': trial.suggest_int('max_depth', 3, 8),\n              'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n              'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0)}  # Search space\n    params.update(base_params)\n    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'valid-aft-nloglik')\n    bst = xgb.train(params, dtrain, num_boost_round=10000,\n                    evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                    early_stopping_rounds=50, verbose_eval=False, callbacks=[pruning_callback])\n    if bst.best_iteration >= 25:\n        return bst.best_score\n    else:\n        return np.inf  # Reject models with < 25 trees\n\n# Run hyperparameter search\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=200)\nprint('Completed hyperparameter tuning with best aft-nloglik = {}.'.format(study.best_trial.value))\nparams = {}\nparams.update(base_params)\nparams.update(study.best_trial.params)\n\n# Re-run training with the best hyperparameter combination\nprint('Re-running the best trial... params = {}'.format(params))\nbst = xgb.train(params, dtrain, num_boost_round=10000,\n                evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                early_stopping_rounds=50)\n\n# Run prediction on the validation set\ndf = pd.DataFrame({'Label (lower bound)': y_lower_bound[valid_index],\n                   'Label (upper bound)': y_upper_bound[valid_index],\n                   'Predicted label': bst.predict(dvalid)})\nprint(df)\n# Show only data points with right-censored labels\nprint(df[np.isinf(df['Label (upper bound)'])])\n\n# Save trained model\nbst.save_model('aft_best_model.json')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}