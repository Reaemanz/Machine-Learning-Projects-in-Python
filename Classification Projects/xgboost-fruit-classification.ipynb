{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-17T15:37:39.107613Z","iopub.execute_input":"2022-05-17T15:37:39.110274Z","iopub.status.idle":"2022-05-17T15:37:39.146375Z","shell.execute_reply.started":"2022-05-17T15:37:39.110094Z","shell.execute_reply":"2022-05-17T15:37:39.145730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\nimport seaborn as sns\nimport tensorflow as tf\nimport pandas as pd\nimport xgboost as xgb\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score","metadata":{"execution":{"iopub.status.busy":"2022-05-17T15:37:39.360596Z","iopub.execute_input":"2022-05-17T15:37:39.362065Z","iopub.status.idle":"2022-05-17T15:37:46.256985Z","shell.execute_reply.started":"2022-05-17T15:37:39.362009Z","shell.execute_reply":"2022-05-17T15:37:46.256129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install openpyxl # We need this to read the excel dataset","metadata":{"execution":{"iopub.status.busy":"2022-05-17T15:37:46.258418Z","iopub.execute_input":"2022-05-17T15:37:46.258696Z","iopub.status.idle":"2022-05-17T15:37:57.484878Z","shell.execute_reply.started":"2022-05-17T15:37:46.258666Z","shell.execute_reply":"2022-05-17T15:37:57.484011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A bit of a good practice is to put the variables here, after importing the libraries. This way it will bea easier to find where to modify the values for our model. ","metadata":{}},{"cell_type":"code","source":"DATA_PATH = '/kaggle/input/date-fruit-datasets/Date_Fruit_Datasets/Date_Fruit_Datasets.xlsx'\nRANDOM_STATE = 42\nLR = 0.01\nTEST_SIZE = 0.33\nMAX_DEPTH = 0\nNTHREAD = 2\nEVAL_METRIC = 'mlogloss'\nBOOSTER = 'gbtree'\nVERBOSITY = 1","metadata":{"execution":{"iopub.status.busy":"2022-05-17T15:37:57.486586Z","iopub.execute_input":"2022-05-17T15:37:57.486826Z","iopub.status.idle":"2022-05-17T15:37:57.492019Z","shell.execute_reply.started":"2022-05-17T15:37:57.486791Z","shell.execute_reply":"2022-05-17T15:37:57.491375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the data\n\nLoad the dataset found in DATA_PATH using pandas. Then gather a bit of info about the dataset and its classes.","metadata":{}},{"cell_type":"code","source":"df = pd.read_excel(DATA_PATH)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-05-17T15:37:57.494144Z","iopub.execute_input":"2022-05-17T15:37:57.495004Z","iopub.status.idle":"2022-05-17T15:37:58.144405Z","shell.execute_reply.started":"2022-05-17T15:37:57.494962Z","shell.execute_reply":"2022-05-17T15:37:58.143432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T15:37:58.145746Z","iopub.execute_input":"2022-05-17T15:37:58.146063Z","iopub.status.idle":"2022-05-17T15:37:58.174923Z","shell.execute_reply.started":"2022-05-17T15:37:58.146029Z","shell.execute_reply":"2022-05-17T15:37:58.173799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Class'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-05-17T15:37:58.176598Z","iopub.execute_input":"2022-05-17T15:37:58.177119Z","iopub.status.idle":"2022-05-17T15:37:58.185210Z","shell.execute_reply.started":"2022-05-17T15:37:58.177071Z","shell.execute_reply":"2022-05-17T15:37:58.184208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{}},{"cell_type":"markdown","source":"After gathering our dataset in the previous step, we need to label encode our target variables. This means that we are going to encode our unique labels (= classes) with values between 0 and n_classes-1. In this case we have 7 unique labels, which means that the we will be getting values from 0 to 6. \n<br>\nThe next step is to split into train and test sets for training. That way we can test our model with unseen data after it finished training.","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\n\ny = df['Class']\ny = le.fit_transform(y) # Encoded labels","metadata":{"execution":{"iopub.status.busy":"2022-04-20T18:04:12.694196Z","iopub.execute_input":"2022-04-20T18:04:12.694429Z","iopub.status.idle":"2022-04-20T18:04:12.706199Z","shell.execute_reply.started":"2022-04-20T18:04:12.694401Z","shell.execute_reply":"2022-04-20T18:04:12.705435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.iloc[:,:-1] # Select all but the last column","metadata":{"execution":{"iopub.status.busy":"2022-04-20T18:04:12.70729Z","iopub.execute_input":"2022-04-20T18:04:12.707715Z","iopub.status.idle":"2022-04-20T18:04:12.718483Z","shell.execute_reply.started":"2022-04-20T18:04:12.70768Z","shell.execute_reply":"2022-04-20T18:04:12.71764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into train test sets\n# TEST_SIZE is the variable which determines the % of the test set we are getting\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    X, \n                                                    y, \n                                                    test_size=TEST_SIZE, \n                                                    random_state=RANDOM_STATE,\n                                                    shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T18:04:12.719782Z","iopub.execute_input":"2022-04-20T18:04:12.720456Z","iopub.status.idle":"2022-04-20T18:04:12.733293Z","shell.execute_reply.started":"2022-04-20T18:04:12.720419Z","shell.execute_reply":"2022-04-20T18:04:12.732671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shape of the dataset\nprint('Shape of training data :',X_train.shape)\nprint('Shape of testing data :',X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T18:04:12.73453Z","iopub.execute_input":"2022-04-20T18:04:12.73483Z","iopub.status.idle":"2022-04-20T18:04:12.747677Z","shell.execute_reply.started":"2022-04-20T18:04:12.734799Z","shell.execute_reply":"2022-04-20T18:04:12.746801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extreme Gradient Boosting Classifier (XGBoost)\nXGBoost is a boosted tree based ensemble classifier which means it works similar to the RandomForest algorithm we have already used at some point of our learning path. What makes this algorithm interesting is that it will automatically reduce the feature set. However, it is known to be a slow algorithm. However, we are going to use a GPU so this should not be a problem. Let's try it out and measure the time of execution.","metadata":{}},{"cell_type":"markdown","source":"## XGBoost algorithm definition\n\nWe define the algorithm choosing the best hyper params we have found after several tests. If you want to find out more about XGBoost, click [here](https://xgboost.readthedocs.io/en/latest/index.html)","metadata":{}},{"cell_type":"code","source":"### Define our XGBoost model with parameters:\n# Variables:\n# random_state: seed to the random generator, so that your train-test splits are always deterministic\n# learning_rate: Step size shrinkage used in update to prevents overfitting. \n#               After each boosting step, we can directly get the weights of new features, \n#               and learning_rate shrinks the feature weights to make the boosting process more conservative.\n# booster: Which booster to use. Can be gbtree, gblinear or dart\n# nthread: default to maximum number of threads available if not set\n# eval_metric: Evaluation metrics for validation data, a default metric will be assigned according to objective \n#               (rmse for regression, and logloss for classification, mean average precision for ranking). Multiple can be used\n# verbosity: printing messages. If 0, nothing is shown\n\nmodel = xgb.XGBClassifier(\n                        random_state=RANDOM_STATE,\n                        learning_rate=LR,\n                        booster=BOOSTER,\n                        nthread=NTHREAD,\n                        eval_metric=EVAL_METRIC,\n                        verbosity=VERBOSITY\n                        )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train\n\nWe want to measure the time the algoritm needs to finish training. We are calculating the time by getting the actual time and the time it is when the training is done. Then we can compute how much time it needed for the execution.","metadata":{}},{"cell_type":"code","source":"start = time.time() # Time before training\n\n# Fit the model with the training data\nmodel.fit(X_train, y_train)\n\nend = time.time() # Time after training\n\n# Compute how much time the model need to train\nprint(f'Training took {round(end-start,2)} seconds to be completed!')","metadata":{"execution":{"iopub.status.busy":"2022-04-20T18:04:12.7605Z","iopub.execute_input":"2022-04-20T18:04:12.761187Z","iopub.status.idle":"2022-04-20T18:04:14.25649Z","shell.execute_reply.started":"2022-04-20T18:04:12.761141Z","shell.execute_reply":"2022-04-20T18:04:14.25557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Getting the metrics\n\nFirst we are going to check the performance of our model by getting the accuracy of the model. Afterwards we are computing the recall and f1-score metrics on the test set. Finally we will plot a confusion matrix to see what labels are failing the most.","metadata":{}},{"cell_type":"code","source":"# predict the target on the train dataset\npredict_train = model.predict(X_train)\nprint('\\nTarget on train data',predict_train) \n \n# Accuray Score on train dataset\naccuracy_train = accuracy_score(y_train,predict_train)\nprint('\\naccuracy_score on train dataset : ', accuracy_train)\n \n# predict the target on the test dataset\npredict_test = model.predict(X_test)\nprint('\\nTarget on test data',predict_test) \n \n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_test,predict_test)\nprint('\\naccuracy_score on test dataset : ', accuracy_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T18:04:14.258959Z","iopub.execute_input":"2022-04-20T18:04:14.259194Z","iopub.status.idle":"2022-04-20T18:04:14.292139Z","shell.execute_reply.started":"2022-04-20T18:04:14.259165Z","shell.execute_reply":"2022-04-20T18:04:14.291283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recall_score = round(recall_score(y_test,predict_test,average='macro'), 2)\nf1_score = round(f1_score(y_test,predict_test,average='macro'), 2)\nprint(f'The accuracy in the test set was {round(accuracy_test, 2)}, the recall was {recall_score} and the f1 score was {f1_score}')","metadata":{"execution":{"iopub.status.busy":"2022-04-20T18:04:14.293411Z","iopub.execute_input":"2022-04-20T18:04:14.293922Z","iopub.status.idle":"2022-04-20T18:04:14.304181Z","shell.execute_reply.started":"2022-04-20T18:04:14.29388Z","shell.execute_reply":"2022-04-20T18:04:14.303577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix = confusion_matrix(y_test, predict_test)\ncm_plot = sns.heatmap(confusion_matrix,\n                      annot=True, \n                      cmap='Blues',\n                      fmt='d');\ncm_plot.set_xlabel('Predicted Values')\ncm_plot.set_ylabel('Actual Values')\ncm_plot.set_title('Confusion Matrix', size=16)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T18:04:14.305232Z","iopub.execute_input":"2022-04-20T18:04:14.306095Z","iopub.status.idle":"2022-04-20T18:04:14.788888Z","shell.execute_reply.started":"2022-04-20T18:04:14.306054Z","shell.execute_reply":"2022-04-20T18:04:14.787992Z"},"trusted":true},"execution_count":null,"outputs":[]}]}