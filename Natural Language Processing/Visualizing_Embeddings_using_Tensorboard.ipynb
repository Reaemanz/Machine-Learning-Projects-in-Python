{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate how you can use Tensorboard to visualize word embeddings which we created in the Training_embeddings_using_gensim.ipynb notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.14.0\n",
      "  Using cached tensorflow-1.14.0-cp36-cp36m-win_amd64.whl (68.3 MB)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch3\\lib\\site-packages (from tensorflow==1.14.0) (0.36.2)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting gast>=0.2.0\n",
      "  Using cached gast-0.5.0-py3-none-any.whl (10 kB)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
      "  Using cached tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.39.0-cp36-cp36m-win_amd64.whl (3.2 MB)\n",
      "Collecting tensorboard<1.15.0,>=1.14.0\n",
      "  Using cached tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
      "Collecting numpy<2.0,>=1.14.5\n",
      "  Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting wrapt>=1.11.1\n",
      "  Using cached wrapt-1.12.1-cp36-cp36m-win_amd64.whl\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch3\\lib\\site-packages (from tensorflow==1.14.0) (1.16.0)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "Collecting protobuf>=3.6.1\n",
      "  Using cached protobuf-3.17.3-cp36-cp36m-win_amd64.whl (910 kB)\n",
      "Collecting astor>=0.6.0\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting h5py\n",
      "  Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (52.0.0.post20210125)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\kumar apurv\\anaconda3\\envs\\ch3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (4.6.1)\n",
      "Collecting dataclasses\n",
      "  Using cached dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Collecting cached-property\n",
      "  Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch3\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch3\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.5.0)\n",
      "Installing collected packages: numpy, dataclasses, cached-property, werkzeug, protobuf, markdown, h5py, grpcio, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, keras-preprocessing, keras-applications, google-pasta, gast, astor, tensorflow\n",
      "Successfully installed absl-py-0.13.0 astor-0.8.1 cached-property-1.5.2 dataclasses-0.8 gast-0.5.0 google-pasta-0.2.0 grpcio-1.39.0 h5py-3.1.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 protobuf-3.17.3 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 werkzeug-2.0.1 wrapt-1.12.1\n",
      "Collecting gensim==3.6.0\n",
      "  Using cached gensim-3.6.0-cp36-cp36m-win_amd64.whl (23.6 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch3\\lib\\site-packages (from gensim==3.6.0) (1.16.0)\n",
      "Collecting scipy>=0.18.1\n",
      "  Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB)\n",
      "Collecting smart-open>=1.2.1\n",
      "  Using cached smart_open-5.1.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch3\\lib\\site-packages (from gensim==3.6.0) (1.19.5)\n",
      "Installing collected packages: smart-open, scipy, gensim\n",
      "Successfully installed gensim-3.6.0 scipy-1.5.4 smart-open-5.1.0\n",
      "Requirement already satisfied: numpy==1.19.5 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch3\\lib\\site-packages (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "!pip install tensorflow==1.14.0\n",
    "!pip install gensim==3.6.0\n",
    "!pip install numpy==1.19.5\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install the requirements for the entire chapter, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# try :\n",
    "#     import google.colab\n",
    "#     !curl https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch3/ch3-requirements.txt | xargs -n 1 -L 1 pip install\n",
    "# except ModuleNotFoundError :\n",
    "#     !pip install -r \"ch3-requirements.txt\"\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the required imports\n",
    "import warnings #ignoring the generated warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "cwd=os.getcwd() \n",
    "model = KeyedVectors.load_word2vec_format(cwd+'\\Models\\word2vec_cbow.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the model's vocabulary size\n",
    "max_size = len(model.wv.vocab)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a numpy array of 0s with the size of the vocabulary and dimensions of our model\n",
    "w2v = np.zeros((max_size,model.wv.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create a new file called metadata.tsv where we save all the words in our model \n",
    "#we also store the embedding of each word in the w2v matrix\n",
    "if not os.path.exists('projections'):\n",
    "    os.makedirs('projections')\n",
    "    \n",
    "with open(\"projections/metadata.tsv\", 'w+',encoding=\"utf-8\") as file_metadata: #changed    added encoding=\"utf-8\"\n",
    "    \n",
    "    for i, word in enumerate(model.wv.index2word[:max_size]):\n",
    "        \n",
    "        #store the embeddings of the word\n",
    "        w2v[i] = model.wv[word]\n",
    "        \n",
    "        #write the word to a file \n",
    "        file_metadata.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing tf session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the tensorflow variable called embeddings that holds the word embeddings:\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    embedding = tf.Variable(w2v, trainable=False, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize all variables\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object of the saver class which is actually used for saving and restoring variables to and from our checkpoints\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with FileWriter,we save summary and events to the event file\n",
    "writer = tf.summary.FileWriter('projections', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the projectors and add the embeddings\n",
    "config = projector.ProjectorConfig()\n",
    "embed= config.embeddings.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify our tensor_name as embedding and metadata_path to the metadata.tsv file\n",
    "embed.tensor_name = 'embedding'\n",
    "embed.metadata_path = 'metadata.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projections/model.ckpt-161017'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the model\n",
    "projector.visualize_embeddings(writer, config)\n",
    "\n",
    "saver.save(sess, 'projections/model.ckpt', global_step=max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a terminal window and type the following command\n",
    "\n",
    "tensorboard --logdir=projections --port=8000\n",
    "\n",
    "If the tensorboard does not work for you try providing the absolute path for projections and re-run the above command\n",
    "\n",
    "If youve done everything right until you will get a link in your terminal through which you can access the tensorboard. Click on the link or copy paste it in your browser. You should see something similar to this.\n",
    "![TensorBoard-1](Images/TensorBoard-1.png)\n",
    "<br>\n",
    "In the top right corner near \"INACTIVE\" click the dropdown arrow. And select PROJECTIONS from te dropdown menu\n",
    "![TensorBoard-2](Images/TensorBoard-2.png)\n",
    "<br>\n",
    "Wait for a few seconds for it to load. You can now see your embeddings there are a lot of setting you can play around and experiment with.\n",
    "![TensorBoard-3](Images/TensorBoard-3.png)\n",
    "<br>\n",
    "Output when we search for a specific word in this case \"human\" and isolate only those points\n",
    "![TensorBoard-4](Images/TensorBoard-4.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
